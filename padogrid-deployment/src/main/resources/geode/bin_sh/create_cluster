#!/usr/bin/env bash

# ========================================================================
# Copyright (c) 2020 Netcrest Technologies, LLC. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========================================================================

SCRIPT_DIR="$(cd -P -- "$(dirname -- "$0")" && pwd -P)"
. $SCRIPT_DIR/.addonenv.sh

# Set PRODUCT here. This is necessary due to the default product read from
# the RWE setenv.sh by the above .addonenv.sh
PRODUCT="geode"

EXECUTABLE="`basename $0`"

__options()
{
   echo "-cluster -type -pod -vm -locator -padoweb -port -?"
}

if [ "$OPTIONS" == "true" ]; then
   __options
   exit
fi

if [ "$HELP" == "true" ]; then
cat <<EOF

WORKSPACE
   $PADOGRID_WORKSPACE

NAME
   $EXECUTABLE - Create a new cluster in the current workspace

SYNOPSIS
   $EXECUTABLE [-cluster cluster_name]
               [-type default|pado|padolite]
               [-pod pod_name]
               [-vm [comma_separated_hosts]]
               [-locator comma_separated_locator_hosts]
               [-padoweb padoweb_host]
               [-port first_locator_port_number] [-?]

DESCRIPTION
   Creates a new cluster in the current workspace. Once the cluster is created,
   you can change the settings in the following file:

   etc/cluster.properties

OPTIONS
   -cluster cluster_name
             Unique cluster name. The cluster name is prepended
             to all member names.

   -type default|pado|padolite
             The 'pado' option creates a Pado cluster that includes full support for grid federation.
             The 'padolite' option creates a normal cluster with Pado enabled allowing Pado tools
             to connect. The 'default' option creates a normal cluster without any Pado dependencies.
             Default: default
  
   -pod pod_name
             Pod name. The 'local' pod is the local machine. This option overrides the '-vm'
             option. 
             Default: local

   -vm [comma_separated_hosts]
             A list of VM hosts or addresses separated by comma. If the list
             contains spaces then enclosed it in quotes. If this option is not
             specified, then the host list defined by VM_HOSTS in the workspace
             'setenv.sh' file is applied.

             If the '-pod' option is specified then this option is suppressed.

   -locator comma_separated_locator_hosts
             A list of locator hosts or addresses separated by comma. If the list
             contains spaces then enclosed it in quotes. This option is meaningful
             only if the '-vm' option is specified. 

             If this option is not specified and the '-pod' option is specified then
             the primary node is used for the locator.

             If this option is not specified but the '-vm' option is specified, then
             the first host from the '-vm' host list is selected for the locator.

   -padoweb padoweb_host
             PadoWeb host name. This option is meaningful only if the '-vm' option
             is specified. If this option is not specified but the '-vm' option is
             specified, then the first host from the '-vm' host list is selected
             for PadoWeb.

   -port first_locator_port_number
             First locator's port number. Port number is incremented
             starting from this port number for the subsquent locators.
             The port number difference between the default locator port
             number and the specified port number is added to the default
             port numbers of other components. For example, if the port
             number 10336 is specified then the difference of 2 (10336-$DEFAULT_LOCATOR_START_PORT)
             is added to the member's default port number of $DEFAULT_MEMBER_START_PORT
             to obtain the port number 40406 for the member's first port number.
             Similarly, the JMX Manager's first HTTP port number would be 7072 ($DEFAULT_JMX_MANAGER_HTTP_START_PORT+2).

             This option applies only for a cluster running locally. It is ignored
             for creating non-local pod and VM clusters.

             Default: $DEFAULT_LOCATOR_START_PORT 

EOF
if [ "$MAN_SPECIFIED" == "false" ]; then
cat <<EOF
DEFAULT
   $EXECUTABLE -cluster $CLUSTER -pod $POD -port $DEFAULT_LOCATOR_START_PORT

FILES
   $PADOGRID_WORKSPACE/setenv.sh
             The current workspace configuration file.

EOF
fi
cat <<EOF
SEE ALSO
EOF
   printSeeAlsoList "*cluster*" $EXECUTABLE
   exit
fi

if [ -z $CLUSTER ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: Cluster name is not specified. Command aborted." 
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

# Abort if the cluster exists
CLUSTER_DIR=$CLUSTERS_DIR/$CLUSTER
if [ -d $CLUSTER_DIR ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: Cluster already exists: [$CLUSTER]. Command aborted."
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

# Suppress pod if VM specified
#if [ "$VM_SPECIFIED" == "true" ]; then
#   POD="local"
#fi

if [ "$POD_SPECIFIED" == "false" ]; then
   POD="local"
fi
if [ "`isPodExist $POD`" == "false" ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: This pod does not exist [$POD]. Command aborted."
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

PADO_TYPE=$TYPE_ARG
if [ "$PADO_TYPE" == "" ]; then
   PADO_TYPE="default"
else
   case "$PADO_TYPE" in
   default|pado|padolite)
      ;;
   *)
      echo -e "${CLightRed}ERROR:${CNone} Invalid '-type' value [$PADO_TYPE]. Command aborted."
      exit 1
      ;;
   esac
fi

if [ "$PADO_TYPE" == "pado" ] || [ "$PADO_TYPE" == "padolite" ]; then
  if [ "$PADO_HOME" == "" ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "ERROR: Pado is not found. You must set PADO_HOME with the Pado installation path in the following"
   echo >&2 "       workspace 'setenv.sh' file."
   echo >&2 -e "       ${CLightRed}$PADOGRID_WORKSPACE/setenv.sh${CNone}"
   echo >&2 "       Command aborted."
   exit 1
  fi
fi

# If -product (hidden option) is specified then make it default. This effectively
# provides a way to create any product clusters in the same workspace.
if [ "$PRODUCT_ARG" != "" ]; then
   PRODUCT=$PRODUCT_ARG
fi

function writeCluster() 
{
   # Update cluster env file with PRODUCT and CLUSTER_TYPE
   updateClusterEnvFile

   # Copy the template gemfire.properties file
   cp $BASE_DIR/etc/template-gemfire.properties $ETC_DIR/gemfire.properties
   # Copy the template cache.xml file
   cp $BASE_DIR/etc/template-cache.xml $ETC_DIR/cache.xml
   # Copy the template cache-spring.xml file
   cp $BASE_DIR/etc/template-cache-spring.xml $ETC_DIR/cache-spring.xml
   # Copy the template cache-spring-padolite.xml file
   cp $BASE_DIR/etc/template-cache-spring-padolite.xml $ETC_DIR/cache-spring-padolite.xml
   # Copy the template application-context.xml file
   cp $BASE_DIR/etc/template-application-context.xml $ETC_DIR/application-context.xml
   # Copy the template log4j2.properties file
   cp $BASE_DIR/etc/template-log4j2.properties $ETC_DIR/log4j2.properties
   # Copy the template prometheus config file
   cp $BASE_DIR/etc/template-prometheus.yml $ETC_DIR/prometheus.yml
   # Copy the template hibernate config file
   cp $BASE_DIR/etc/template-hibernate.cfg-mysql.xml $ETC_DIR/hibernate.cfg-mysql.xml
   cp $BASE_DIR/etc/template-hibernate.cfg-postgresql.xml $ETC_DIR/hibernate.cfg-postgresql.xml

   # Copy clusters/bin_sh
   cp -r $BASE_DIR/clusters/* $CLUSTER_DIR/
   if [ "$PADO_TYPE" == "padolite" ]; then
      cp -r $BASE_DIR/clusters_padolite/* $CLUSTER_DIR/
      mkdir -p $CLUSTER_DIR/pado/db/vp
      mkdir -p $CLUSTER_DIR/pado/security
      if [ "$PADO_HOME" != "" ] && [ -d "$PADO_HOME/etc" ]; then
          cp $PADO_HOME/etc/pado*.dtd $CLUSTER_DIR/pado/
      fi
      # Place Pado biz jars in the plugins directory, which serves as the IBiz repo.
      cp $PADO_HOME/plugins/* $CLUSTER_DIR/plugins/

      # Generate pado.xml
      sed -e "s/\${GRID_ID}/$CLUSTER/g" \
          $BASE_DIR/clusters_padolite/pado/etc/template-pado.xml \
          | tr '|' '\n' \
          > $CLUSTER_DIR/pado/etc/pado.xml

      # Generate cache.xml
      sed -e "s/\${PADO_INITIALIZER}/\t<\\!-- @PADOGRID-START (marker generated and used by PadoGrid) -->\n\t<\\!-- Pado Initializer - Enables PadoLite -->\n\t<initializer>\n\t\t<class-name>com.netcrest.pado.gemfire.GemfirePadoServerInitializer<\/class-name>\n\t<\/initializer>\n\t<\\!-- @PADOGRID-END -->/" \
          $BASE_DIR/clusters_padolite/etc/template-cache.xml \
          | tr '|' '\n' \
          > $CLUSTER_DIR/etc/cache.xml
   fi

   # Create bin_sh/setenv.sh
   echo "#" > $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Add cluster specific environment variables in this file." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# The 'padolite.enabled' property in 'etc/cluster.properties' enables"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# PadoLite in cluster members."  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "PADOLITE_ENABLED=\$(getClusterProperty \"padolite.enabled\" \"false\")"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# The 'spring.bootstrap.enabled' property in 'etc/cluster.properties'"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# enables bootstrapping cache servers using Spring."  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "SPRING_BOOTSTRAP_ENABLED=\$(getClusterProperty \"spring.bootstrap.enabled\" \"false\")"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "if [ \"\$SPRING_BOOTSTRAP_ENABLED\" == \"true\" ]; then"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   if [ \"\$PADOLITE_ENABLED\" == \"true\" ]; then"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "      CONFIG_FILE=\$ETC_DIR/cache-spring-padolite.xml"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   else"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "      CONFIG_FILE=\$ETC_DIR/cache-spring.xml"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   fi"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   JAVA_OPTS=\"\$JAVA_OPTS --J=-Dgeode-addon.application.context.file=application-context.xml\""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   CLASSPATH="\$CLASSPATH:\$ETC_DIR/""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "fi"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Enable Pado if defined in setenv_pado.sh"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "if [ -f "\$CLUSTER_DIR/pado/bin_sh/setenv_pado.sh" ]; then"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "   . \$CLUSTER_DIR/pado/bin_sh/setenv_pado.sh"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "fi"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Set Java options, i.e., --J=-Dproperty=xyz" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Some useful Geode/GemFire system properties:"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Always keep values serialized. By default, once deserialized it remains deserialized. Default: false "  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\$JAVA_OPTS --J=-Dgemfire.PREFER_SERIALIZED=true"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Enable invocation of listeners in both primary and secondary buckets. Default: false"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\$JAVA_OPTS --J=-Dgemfire.BucketRegion.alwaysFireLocalListeners=true"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# IMPORTANT:"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    If you are running on Windows, then you must convert the file paths from Unix notations"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    Windows notations. For example,"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# HIBERNATE_CONFIG_FILE=\"\$CLUSTER_DIR/etc/hibernate.cfg-mysql.xml\""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# if [[ \${OS_NAME} == CYGWIN* ]]; then"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    HIBERNATE_CONFIG_FILE=\"\$(cygpath -wp \"\$HIBERNATE_CONFIG_FILE\")\""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# fi"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# JAVA_OPTS=\"\$JAVA_OPTS --J=-Dgeode-addon.hibernate.config=\$HIBERNATE_CONFIG_FILE\""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# To use Hibernate backed CacheWriterLoaderPkDbImpl, set the following property and" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# configure CacheWriterLoaderPkDbImpl in the \$CLUSTER_DIR/etc/cache.xml file." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# MySQL and PostgreSQL Hibernate configuration files are provided to get" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# you started. You should copy one of them and enter your DB information." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# You can include your JDBC driver in the ../pom.xml file and run ./build_app" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# which downloads and places it in the \$PADOGRID_WORKSPACE/lib" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# directory. CLASSPATH includes all the jar files in that directory for" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# the apps and clusters running in this workspace." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\"\$JAVA_OPTS --J=-Dgeode-addon.hibernate.config=\$CLUSTER_DIR/etc/hibernate.cfg-mysql.xml\"" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\"\$JAVA_OPTS --J=-Dgeode-addon.hibernate.config=\$CLUSTER_DIR/etc/hibernate.cfg-postgresql.xml\"" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Set RUN_SCRIPT. Absolute path required." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# If set, the 'start_member' command will run this script instead of 'gfsh start server'." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Your run script will inherit the following:" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    JAVA      - Java executable."  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    JAVA_OPTS - Java options set by padogrid." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    CLASSPATH - Class path set by padogrid. You can include additional libary paths." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#                You should, however, place your library files in the plugins directories" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#                if possible." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#  CLUSTER_DIR - This cluster's top directory path, i.e., $PADOGRID_WORKSPACE/clusters/$CLUSTER" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Run Script Example:" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    \"\$JAVA\" \$JAVA_OPTS com.newco.MyMember &" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Although it is not required, your script should be placed in the bin_sh directory." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#RUN_SCRIPT=\$CLUSTER_DIR/bin_sh/your-script" >> $CLUSTER_DIR/bin_sh/setenv.sh

   
   # Create $ETC_DIR/cluster.properties
   echo "# padogrid cluster properties." > $ETC_DIR/cluster.properties
   echo "# These properties pertain to all of the members belonging to the named cluster below." >> $ETC_DIR/cluster.properties
   echo "# Cluster Creation Date: `date`" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Cluster name
   echo "# Cluster name" >> $ETC_DIR/cluster.properties
   echo "cluster.name=$CLUSTER" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Standalone
   echo "# If standalone is enabled (true), then the cache server runs as a stanalone server" >> $ETC_DIR/cluster.properties
   echo "# and locators are ignored." >> $ETC_DIR/cluster.properties
   echo "standalone.enabled=false" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # hostname-for-clients
   echo "# The following properties are used only if the cluster is running on" >> $ETC_DIR/cluster.properties
   echo "# the local machine. For VM clusters, each locator and member must include" >> $ETC_DIR/cluster.properties
   echo "# their own properties." >> $ETC_DIR/cluster.properties
   echo "cluster.bindAddress=$BIND_ADDRESS" >> $ETC_DIR/cluster.properties
   echo "cluster.hostnameForClients=$HOSTNAME_FOR_CLIENTS" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Pod
   echo "# Pod properties" >> $ETC_DIR/cluster.properties
   echo "pod.name=$POD" >> $ETC_DIR/cluster.properties
   echo "pod.type=$POD_TYPE" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Locator TCP port
   echo "# Locator TCP start port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "locator.tcp.startPort=$NEW_LOCATOR_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Member server TCP port
   echo "# Member TCP start port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "tcp.startPort=$NEW_MEMBER_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Member REST API HTTP port
   echo "# Enable/disable member REST API" >> $ETC_DIR/cluster.properties
   echo "http.enabled=$DEFAULT_MEMBER_HTTP_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# Member REST API HTTP start port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "http.startPort=$NEW_MEMBER_HTTP_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Debug
   echo "# Enable/disable locator debugging" >> $ETC_DIR/cluster.properties
   echo "locator.debug.enabled=$DEFAULT_LOCATOR_DEBUG_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first locator's debug port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "locator.debug.startPort=$NEW_LOCATOR_DEBUG_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   echo "# Enable/disable member debugging" >> $ETC_DIR/cluster.properties
   echo "debug.enabled=$DEFAULT_DEBUG_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's debug port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "debug.startPort=$NEW_DEBUG_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # JMX
   echo "# Enable/disable locator jmx" >> $ETC_DIR/cluster.properties
   echo "locator.jmx.enabled=$DEFAULT_LOCATOR_JMX_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first locator's JMX port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "locator.jmx.startPort=$NEW_LOCATOR_JMX_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   echo "# Enable/disable member jmx" >> $ETC_DIR/cluster.properties
   echo "jmx.enabled=$DEFAULT_JMX_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's JMX port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "jmx.startPort=$NEW_JMX_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Prometheus
   echo "# Enable/disable locator Prometheus" >> $ETC_DIR/cluster.properties
   echo "locator.prometheus.enabled=$DEFAULT_LOCATOR_PROMETHEUS_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first locator's PROMETHEUS port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "locator.prometheus.startPort=$NEW_LOCATOR_PROMETHEUS_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   echo "# Enable/disable member Prometheus" >> $ETC_DIR/cluster.properties
   echo "prometheus.enabled=$DEFAULT_PROMETHEUS_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's PROMETHEUS port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "prometheus.startPort=$NEW_PROMETHEUS_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Heap
   echo "# Locator heap min and max values" >> $ETC_DIR/cluster.properties
   echo "locator.heap.min=$DEFAULT_LOCATOR_MIN_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "locator.heap.max=$DEFAULT_LOCATOR_MAX_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   echo "# Member heap min and max values" >> $ETC_DIR/cluster.properties
   echo "heap.min=$DEFAULT_MIN_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "heap.max=$DEFAULT_MAX_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # JMX Manager (Pulse)
   echo "# JMX Manager port numbers" >> $ETC_DIR/cluster.properties
   echo "locator.jmx.manager.http.startPort=$NEW_JMX_MANAGER_HTTP_START_PORT" >> $ETC_DIR/cluster.properties
   echo "locator.jmx.manager.startPort=$NEW_JMX_MANAGER_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # GC logging
   echo "# GC logging flag. If true then GC information is logged." >> $ETC_DIR/cluster.properties
   echo "gc.log.enabled=$DEFAULT_GC_LOG_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# Default GC log file flag. If true then GC information is logged in a separate log file." >> $ETC_DIR/cluster.properties
   echo "gc.log.file.enabled=$DEFAULT_GC_LOG_FILE_ENABLED" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Spring
   echo "# Enable/disable Spring to bootstrap members. If enabled (true), then" >> $ETC_DIR/cluster.properties
   echo "# 'etc/application-context.xml' configures members. Note that Spring Data" >> $ETC_DIR/cluster.properties
   echo "# for Geode/GemFire is not part of PadoGrid distributions. If this property" >> $ETC_DIR/cluster.properties
   echo "# is set to "true", then you must first run 'bin_sh/build_app' to download" >> $ETC_DIR/cluster.properties
   echo "# the required 'spring-data-gemfire' artifact before starting the cluster." >> $ETC_DIR/cluster.properties
   echo "spring.bootstrap.enabled=false" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Pado
   echo "# --- Pado properties." >> $ETC_DIR/cluster.properties
   echo "# Pado properties have no effect unless PadoLite is enabled or this cluster" >> $ETC_DIR/cluster.properties
   echo "# is a full-blown Pado grid." >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   echo "# PadoLite" >> $ETC_DIR/cluster.properties
   echo "# Enable/disable PadoLite. If enabled (true), then a Pado installation is required." >> $ETC_DIR/cluster.properties
   if [ "$PADO_TYPE" == "padolite" ]; then
      echo "padolite.enabled=true" >> $ETC_DIR/cluster.properties
   else
      echo "padolite.enabled=false" >> $ETC_DIR/cluster.properties
   fi
   echo "" >> $ETC_DIR/cluster.properties
   echo "# PadoWeb" >> $ETC_DIR/cluster.properties
   echo "# PadoWeb properties are for PadoWeb only. Cluster members do not use them." >> $ETC_DIR/cluster.properties
   echo "padoweb.host=$PADOWEB_HOST" >> $ETC_DIR/cluster.properties
   echo "padoweb.http.port=$NEW_PADOWEB_HTTP_PORT" >> $ETC_DIR/cluster.properties
   echo "padoweb.https.port=$NEW_PADOWEB_HTTPS_PORT" >> $ETC_DIR/cluster.properties
   echo "padoweb.https.enabled=false" >> $ETC_DIR/cluster.properties
   echo "padoweb.contextPath=$DEFAULT_PADOWEB_CONTEXT_PATH" >> $ETC_DIR/cluster.properties
   echo "# ---" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # VM
   echo "# Enable/disable VM cluster" >> $ETC_DIR/cluster.properties
   echo "vm.enabled=$VM_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# A comma separated list of host names or addresses. IMPORTANT: No spaces allowed." >> $ETC_DIR/cluster.properties

   # Replace all spaces from host lists
   VM_LOCATOR_HOSTS=$(echo "$VM_LOCATOR_HOSTS" | sed "s/ //g")
   VM_HOSTS=$(echo "$VM_HOSTS" | sed "s/ //g")
   echo "vm.locator.hosts=$VM_LOCATOR_HOSTS" >> $ETC_DIR/cluster.properties
   echo "vm.hosts=$VM_HOSTS" >> $ETC_DIR/cluster.properties
   echo "# SSH user name. If not specified then defaults to the shell login session user name." >> $ETC_DIR/cluster.properties
   echo "vm.user=$VM_USER" >> $ETC_DIR/cluster.properties
   echo "# Optional private key file path. You may use the private key file for AWS EC2, for example. " >> $ETC_DIR/cluster.properties
   if [ "$VM_PRIVATE_KEY_FILE" == "" ]; then
      echo "#vm.privateKeyFile=$VM_KEY_FILE" >> $ETC_DIR/cluster.properties
   else
      echo "vm.privateKeyFile=$VM_PRIVATE_KEY_FILE" >> $ETC_DIR/cluster.properties
   fi

   echo "" >> $ETC_DIR/cluster.properties

   if [ "$VM_ENABLED" == "true" ]; then
      echo "# Individual VM host settings: vm.<host>.*" >> $ETC_DIR/cluster.properties
      __VM_HOSTS=$(echo "$VM_HOSTS" | sed "s/,/ /g")
      for VM_HOST in $__VM_HOSTS; do
         echo "#vm.${VM_HOST}.bindAddress=${VM_HOST}" >> $ETC_DIR/cluster.properties
         echo "vm.${VM_HOST}.hostnameForClients=${VM_HOST}" >> $ETC_DIR/cluster.properties
         echo "vm.${VM_HOST}.redundancyZone=" >> $ETC_DIR/cluster.properties
      done
   fi

   # Pado
   if [ "$PADO_TYPE" == "pado" ]; then
      cp -r $PADO_HOME/data $CLUSTER_DIR/
      cp -r $PADO_HOME/db $CLUSTER_DIR/
      cp -r $PADO_HOME/etc/pado1_0.dtd $ETC_DIR/
      cp -r $PADO_HOME/etc/client $ETC_DIR/
      cp -r $PADO_HOME/lang $CLUSTER_DIR/
      cp -r $PADO_HOME/ldap $CLUSTER_DIR/
      cp -r $PADO_HOME/script $CLUSTER_DIR/
      cp -r $PADO_HOME/security $CLUSTER_DIR/
      sed "s/mygrid/$CLUSTER/g" $PADO_HOME/data/template/scheduler/etc/db-example.json > $CLUSTER_DIR/data/template/scheduler/etc/db-example.json
      sed "s/mygrid/$CLUSTER/g" $PADO_HOME/data/template/scheduler/README_SCHEDULER.txt > $CLUSTER_DIR/data/template/scheduler/README_SCHEDULER.txt
      sed "s/mygrid/$CLUSTER/g" $PADO_HOME/ldap/example/newco-all.ldif > $CLUSTER_DIR/ldap/example/newco-all.ldif
      mv $CLUSTER_DIR/security/client/mygrid-user.keystore $CLUSTER_DIR/security/client/$CLUSTER-user.keystore
      mv $CLUSTER_DIR/data/template/scheduler/schema/mygrid-grid_path.schema $CLUSTER_DIR/data/template/scheduler/schema/$CLUSTER-grid_path.schema
      # Pado specifics maintained by PadoGrid
      cp -r $PADOGRID_HOME/geode/pado/* $CLUSTER_DIR/
      cp -r $PADOGRID_HOME/geode/pado/etc/grid/server.xml $CLUSTER_DIR/etc/cache.xml

      # Generate pado.xml
      sed -e "s/\${GRID_ID}/$CLUSTER/g" \
          $PADOGRID_HOME/geode/pado/etc/grid/template-pado.xml \
          | tr '|' '\n' \
          > $CLUSTER_DIR/etc/grid/pado.xml
   fi
}

# 
# VM cluster
#
if [ "$VM_SPECIFIED" == "true" ] && [ "$POD" == "local" ]; then
   VM_ENABLED="true"
fi
if [ "$VM_USER" == "" ]; then
   VM_USER=$(whoami)
fi
VM_KEY_FILE="/your/private/keyfile.pem"
if [ "$VM_ENABLED" == "true" ]; then
   if [ "$VM_HOSTS_ARG" != "" ]; then
      VM_HOSTS="$VM_HOSTS_ARG"
   fi

   # Default locator host
   if [ "$LOCATOR" == "" ]; then
      # Extract the first host from the VM host list
      VM_LOCATOR_HOSTS=${VM_HOSTS%%,*}
   else
      VM_LOCATOR_HOSTS=$LOCATOR
   fi
   if [ "$VM_LOCATOR_HOSTS" == "" ]; then
      echo "WARNING: VM locator host\(s\) undefined. Configuring with this host."
      VM_LOCATOR_HOSTS=`hostname`
   fi
   
   # PadoWeb
   if [ "$PADOWEB" == "" ]; then
      # Extract the first host from the VM host list
      VM_PADOWEB_HOSTS=${VM_HOSTS%%,*}
   else
      VM_PADOWEB_HOSTS=$PADOWEB
   fi
   if [ "$VM_PADOWEB_HOSTS" == "" ]; then
      echo "WARNING: VM padoweb host\(s\) undefined. Configuring with this host."
      VM_PADOWEB_HOSTS=`hostname`
   fi

   if [ "$VM_HOSTS" == "" ]; then
      echo "WARNING: VM hosts undefined. Configuring with this host."
      VM_HOSTS=`hostname`
   fi
else
   VM_LOCATOR_HOSTS=`hostname`
   VM_PADOWEB_HOSTS=`hostname`
   VM_HOSTS=`hostname`
fi

# Collect all clusters
pushd $CLUSTERS_DIR > /dev/null 2>&1
CLUSTERS=""
COUNT=0
for i in *; do
   if [ -d "$i" ]; then
      let COUNT=COUNT+1
      if [ $COUNT -eq 1 ]; then
         CLUSTERS="$i"
      else
         CLUSTERS="$CLUSTERS $i"
      fi
   fi
done
popd > /dev/null 2>&1

# If the port number is not specified, or if there aren't any existing clusters
# or the pod name is not "local" then use the default ports. For non-local pods,
# we can ignore the port number conflicts.
if [ "$PORT_ARG" == "" ] || [ "$CLUSTER" == "" ] || [ "$POD" != "local" ] || [ "$VM_ENABLED" == "true" ]; then
   NEW_LOCATOR_START_PORT=$DEFAULT_LOCATOR_START_PORT
   NEW_MEMBER_START_PORT=$DEFAULT_MEMBER_START_PORT
   NEW_MEMBER_HTTP_START_PORT=$DEFAULT_MEMBER_HTTP_START_PORT
   NEW_LOCATOR_DEBUG_START_PORT=$DEFAULT_LOCATOR_DEBUG_START_PORT
   NEW_DEBUG_START_PORT=$DEFAULT_DEBUG_START_PORT
   NEW_LOCATOR_JMX_START_PORT=$DEFAULT_LOCATOR_JMX_START_PORT
   NEW_JMX_START_PORT=$DEFAULT_JMX_START_PORT
   NEW_LOCATOR_PROMETHEUS_START_PORT=$DEFAULT_LOCATOR_PROMETHEUS_START_PORT
   NEW_PROMETHEUS_START_PORT=$DEFAULT_PROMETHEUS_START_PORT
   NEW_JMX_MANAGER_HTTP_START_PORT=$DEFAULT_JMX_MANAGER_HTTP_START_PORT
   NEW_JMX_MANAGER_START_PORT=$DEFAULT_JMX_MANAGER_START_PORT
   NEW_PADOWEB_HTTP_PORT=$DEFAULT_PADOWEB_HTTP_PORT
   NEW_PADOWEB_HTTPS_PORT=$DEFAULT_PADOWEB_HTTPS_PORT
else
   NEW_LOCATOR_START_PORT=$PORT_ARG
   let PORT_DIFF=$NEW_LOCATOR_START_PORT-DEFAULT_LOCATOR_START_PORT
   let NEW_MEMBER_START_PORT=DEFAULT_MEMBER_START_PORT+PORT_DIFF
   let NEW_MEMBER_HTTP_START_PORT=DEFAULT_MEMBER_HTTP_START_PORT+PORT_DIFF
   let NEW_LOCATOR_DEBUG_START_PORT=DEFAULT_LOCATOR_DEBUG_START_PORT+PORT_DIFF
   let NEW_DEBUG_START_PORT=DEFAULT_DEBUG_START_PORT+PORT_DIFF
   let NEW_LOCATOR_PROMETHEUS_START_PORT=DEFAULT_LOCATOR_PROMETHEUS_START_PORT+PORT_DIFF
   let NEW_PROMETHEUS_START_PORT=DEFAULT_PROMETHEUS_START_PORT+PORT_DIFF
   let NEW_LOCATOR_JMX_START_PORT=DEFAULT_LOCATOR_JMX_START_PORT+PORT_DIFF
   let NEW_JMX_START_PORT=DEFAULT_JMX_START_PORT+PORT_DIFF
   let NEW_JMX_MANAGER_HTTP_START_PORT=DEFAULT_JMX_MANAGER_HTTP_START_PORT+PORT_DIFF
   let NEW_JMX_MANAGER_START_PORT=DEFAULT_JMX_MANAGER_START_PORT+PORT_DIFF
   let NEW_PADOWEB_HTTP_PORT=DEFAULT_PADOWEB_HTTP_PORT+PORT_DIFF
   let NEW_PADOWEB_HTTPS_PORT=DEFAULT_PADOWEB_HTTPS_PORT+PORT_DIFF
fi

# Create the cluster sub-directories and files.
LOG_DIR=$CLUSTERS_DIR/$CLUSTER/log
ETC_DIR=$CLUSTERS_DIR/$CLUSTER/etc
LIB_DIR=$CLUSTERS_DIR/$CLUSTER/lib
PLUGINS_DIR=$CLUSTERS_DIR/$CLUSTER/plugins
CONFIG_FILE=$CLUSTERS_DIR/$CLUSTER/etc/cache.xml
mkdir -p $ETC_DIR
mkdir -p $LOG_DIR
mkdir -p $LIB_DIR
mkdir -p $PLUGINS_DIR

# Copy the plugin jars in the cluster plugins directory.
# All plugins will be hot-deployed to the cluster plugins directory.
if [ "$PADO_TYPE" == "pado" ]; then
   if [ -d "$PADO_HOME/plugins" ]; then
      cp -r "$PADO_HOME/plugins"/* "$PLUGINS_DIR"/
   fi
fi

# Add one (1) member if VM is enabled.
# Add one (1) locator if the pod name is "local".
# Add two (2) members if the pod name is "local".
# For non-local clusters with remote members, we start with an empty cluster and
# allow the user to identify the VMs to run the members.
BIND_ADDRESS="localhost"
HOSTNAME_FOR_CLIENTS="localhost"
if [ "$VM_ENABLED" == "true" ]; then
   MEMBER_1=`getMemberName 1`
   MEMBER_1_DIR=$CLUSTER_DIR/run/$MEMBER_1
   mkdir -p $MEMBER_1_DIR
   if [ "POD" == "local" ]; then
      POD_TYPE="local"
   fi
   PADOWEB_HOST=$VM_PADOWEB_HOSTS
elif [ "$POD" == "local" ]; then
   LOCATOR_1=`getLocatorName 1`
   LOCATOR_1_DIR=$CLUSTER_DIR/run/$LOCATOR_1
   mkdir -p $LOCATOR_1_DIR
   MEMBER_1=`getMemberName 1`
   MEMBER_2=`getMemberName 2`
   MEMBER_1_DIR=$CLUSTER_DIR/run/$MEMBER_1
   MEMBER_2_DIR=$CLUSTER_DIR/run/$MEMBER_2
   mkdir -p $MEMBER_1_DIR
   mkdir -p $MEMBER_2_DIR
   POD_TYPE="local" 
   PADOWEB_HOST="localhost"
fi

if [ "$POD" != "local" ]; then

   # TODO: Pod VM_ENABLED is currently for workspace-wide only. To support individual clusters,
   #       VM_* parameters must be set for each cluster. Significant overhauling may required.
   # VM_ENABLED="true"

   AVAHI_ENABLED=`getPodProperty "pod.avahi.enabled" false`
   NODE_COUNT=`getPodProperty "node.count" "0"`
   VM_USER="vagrant"
   POD_TYPE="vagrant"
   VM_HOSTS=""
   
   PRIMARY_NODE_NAME=`getPodProperty "node.name.primary" "$DEFAULT_NODE_NAME_PRIMARY"`
   PRIMARY_NODE_LOCAL="${PRIMARY_NODE_NAME}.local"
   BIND_ADDRESS=$PRIMARY_NODE_LOCAL
   HOSTNAME_FOR_CLIENTS=$PRIMARY_NODE_LOCAL

   # Create locator run directory
   NODE_NAME_PREFIX=$PRIMARY_NODE_NAME
   LOCATOR_1=`getLocatorName 1`
   LOCATOR_1_DIR=$CLUSTER_DIR/run/$LOCATOR_1
   mkdir -p $LOCATOR_1_DIR

   # Create member run directories
   NODE_NAME_PREFIX=`getPodProperty "node.name.prefix" $NODE_NAME_PREFIX`
   for i in $(seq 1 $NODE_COUNT); do
       MEMBER=`getMemberName $i`
       MEMBER_DIR=$CLUSTER_DIR/run/$MEMBER
       mkdir -p $MEMBER_DIR
   done
   
   # Vagrant pod VMs   
   if [ "$AVAHI_ENABLED" == "true" ]; then
      NODE_NAME_PRIMARY=`getPodProperty "node.name.primary"`
      if [ "$NODE_NAME_PRIMARY" != "" ]; then
         NODE_NAME_PRIMARY=${NODE_NAME_PRIMARY}.local
      else
         NODE_NAME_PRIMARY="localhost"
      fi
      PADOWEB_HOST=$NODE_NAME_PRIMARY

      for i in $(seq 1 $NODE_COUNT); do
        __MEMBER_NUM=`getMemberNumWithLeadingZero $i`
        NODE_LOCAL="${NODE_NAME_PREFIX}-${__MEMBER_NUM}.local"
        if [ $i -eq 1 ]; then
           VM_HOSTS="$NODE_LOCAL"
        else
           VM_HOSTS="$VM_HOSTS,$NODE_LOCAL"
        fi
      done
      if [ "$LOCATOR_SPECIFIED" == "true" ]; then
         VM_LOCATOR_HOSTS=$LOCATOR
      else
         # pnode
         VM_LOCATOR_HOSTS=$PRIMARY_NODE_LOCAL
      fi
   else
      POD_DIR="$PADOGRID_WORKSPACE/pods/$POD"
      POD_SETENV="$POD_DIR/bin_sh/setenv.sh"
      HOST_PRIVATE_IP=""
      if [ -f "$POD_SETENV" ]; then
         . $POD_SETENV
      fi
      if [ "$HOST_PRIVATE_IP" == "" ]; then
         AVAILABLE_IP_ADDRESSES=`getPrivateNetworkAddresses`
         if [ "$AVAILABLE_IP_ADDRESSES" == "" ]; then
            echo >&2 "ERROR: Private IP address not specified and unable to determine. Please add"
            echo >&2 "       a host-only ethernet adapter to VirtualBox. Command aborted."
            exit 1
         fi
         # Pick the first one in the list
         for i in $AVAILABLE_IP_ADDRESSES; do
            HOST_PRIVATE_IP=$i
            break;
         done
      fi
      IP_LAST_OCTET=`getPodProperty "node.ip.lastOctet" $IP_LAST_OCTET`
      # Extract the first three octets from the primary IP.
      BASE_PRIVATE_IP_FIRST_THREE_OCTETS=${HOST_PRIVATE_IP%.*}
      BASE_PRIVATE_IP_LAST_OCTET=$IP_LAST_OCTET
      BASE_PRIVATE_IP=${BASE_PRIVATE_IP_FIRST_THREE_OCTETS}.${BASE_PRIVATE_IP_LAST_OCTET}
      NODE_NAME_PRIMARY=${BASE_PRIVATE_IP}
      PADOWEB_HOSTS=$NODE_NAME_PRIMARY
      for i in $(seq 1 $NODE_COUNT); do
         let NODE_OCTET=i+BASE_PRIVATE_IP_LAST_OCTET
         NODE_IP=${BASE_PRIVATE_IP_FIRST_THREE_OCTETS}.${NODE_OCTET}
         if [ $i -eq 1 ]; then
            VM_HOSTS="$NODE_IP"
         else
            VM_HOSTS="$VM_HOSTS,$NODE_IP"
         fi
      done   
      if [ "$LOCATOR_SPECIFIED" == "true" ]; then
         VM_LOCATOR_HOSTS=$LOCATOR
      else
         # pnode
         VM_LOCATOR_HOSTS=$NODE_NAME_PRIMARY
      fi
   fi

fi

# Write cluster files
writeCluster

# Display cluster information
echo "----------------------------------------------------------------"
echo "                  WORKSPACE: $PADOGRID_WORKSPACE"
echo "            Cluster Created: $CLUSTER"
echo "               CLUSTER_TYPE: $CLUSTER_TYPE"
echo "                  PADO_TYPE: $PADO_TYPE"
if [ "$VM_ENABLED" == "true" ]; then
echo "                 Deployment: VM"
else
echo "                        POD: $POD"
fi
echo "                CLUSTER_DIR: $CLUSTER_DIR"
echo "                 CONFIG_DIR: $ETC_DIR"
echo "                CONFIG_FILE: $CONFIG_FILE"
echo "                    LOG_DIR: $LOG_DIR"
echo "         LOCATOR_START_PORT: $NEW_LOCATOR_START_PORT"
echo "          MEMBER_START_PORT: $NEW_MEMBER_START_PORT"
echo "     MEMBER_HTTP_START_PORT: $NEW_MEMBER_HTTP_START_PORT"
echo "           DEBUG_START_PORT: $NEW_DEBUG_START_PORT"
echo "             JMX_START_PORT: $NEW_JMX_START_PORT"
echo "JMX_MANAGER_HTTP_START_PORT: $NEW_JMX_MANAGER_HTTP_START_PORT"
echo "     JMX_MANAGER_START_PORT: $NEW_JMX_MANAGER_START_PORT"
echo "      PROMETHEUS_START_PORT: $NEW_PROMETHEUS_START_PORT"
if [ "$PADO_TYPE" == "pado" ] || [ "$PADO_TYPE" == "padolite" ]; then
echo "          PADOWEB_HTTP_PORT: $NEW_PADOWEB_HTTP_PORT"
echo "         PADOWEB_HTTPS_PORT: $NEW_PADOWEB_HTTPS_PORT"
fi
echo ""
echo "The default config.xml file has been placed in the CONFIG_DIR directory."
echo "Modify or replace it with the appropriate file."
echo ""
if [ "$VM_ENABLED" == "true" ] && [ "$POD" == "local" ]; then
   echo "One (1) member has been configured (added) to run in this cluster as follows:"
   echo ""
   echo "      Member: $MEMBER_1"
   echo " Working Dir: $MEMBER_1_DIR"
   echo ""
elif [ "$POD" == "local" ]; then
   echo "One (1) locator and two (2) members have been configured (added) to run in this cluster as follows:"
   echo ""
   echo "     Locator: $LOCATOR_1"
   echo " Working Dir: $LOCATOR_1_DIR"
   echo "      Member: $MEMBER_1"
   echo " Working Dir: $MEMBER_1_DIR"
   echo "      Member: $MEMBER_2"
   echo " Working Dir: $MEMBER_2_DIR"
   echo ""
else
   echo "$NODE_COUNT members have been configured (added) to run in this cluster as follows:"
   echo ""
   NUM=0
   NODE_NAME_PREFIX=$PRIMARY_NODE_NAME
   VM_LOCATOR_HOSTS=$(echo "$VM_LOCATOR_HOSTS" | sed "s/,/ /g")
   for i in $VM_LOCATOR_HOSTS; do
       let NUM=NUM+1
       NODE_NAME_PREFIX=$i
       LOCATOR=`getLocatorName $NUM`
       LOCATOR_DIR=$CLUSTER_DIR/run/$LOCATOR
       echo "     Locator: $LOCATOR"
       echo " Working Dir: $LOCATOR_DIR"
   done
   NODE_NAME_PREFIX=`getPodProperty "node.name.prefix" $NODE_NAME_PREFIX`
   for i in $(seq 1 $NODE_COUNT); do
       MEMBER=`getMemberName $i`
       MEMBER_DIR=$CLUSTER_DIR/run/$MEMBER
      echo "      Member: $MEMBER"
      echo " Working Dir: $MEMBER_DIR"
   done
   echo ""
fi
echo "1. Update Geode/GemFire configuration in cache.xml as needed:"
echo "      ${ETC_DIR}/cache.xml"
echo "2. Place your application jar files in one of the 'plugins' directories:"
echo "      $PADOGRID_WORKSPACE/plugins/"
echo "      $CLUSTER_DIR/plugins/"
echo "3. Configure cluster properties in:"
echo "      $ETC_DIR/cluster.properties"
if [ "$VM_ENABLED" == "true" ]; then
   echo "4. You must configure password-less ssh login to all VMs. On AWS, this"
   echo "   is already configured but you will need to enter the correct private key"
   echo "   file path in the cluster.properties file."
   echo "5. To change VM host names, list the VM host names in the cluster.properties file"
   echo "   for locators and members."
   echo "      vm.locator.hosts=host1,host2,..."
   echo "      vm.hosts=host1,host2,..."
   echo "6. Deploy the cluster configuration changes you made to all the VM hosts listed in the"
   echo "   previous step by running 'vm_copy' or 'vm_sync'."
   echo "      vm_copy $CLUSTER_DIR"
   echo "7. To start the cluster:"
   echo "      start_cluster -cluster $CLUSTER"
   echo "8. To verify the cluster state:"
   echo "      show_cluster -cluster $CLUSTER"
   echo "9. To view log file:"
   echo "      show_log -cluster $CLUSTER" 
elif [ "$POD" == "local" ]; then 
   echo "4. To configure additional members:"
   echo "      add_member -cluster $CLUSTER"
   echo "5. To start the cluster: "
   echo "      start_cluster -cluster $CLUSTER"
   echo "6. To verify the cluster state:"
   echo "      show_cluster -cluster $CLUSTER"
   echo "7. To view log file:"
   echo "      show_log -cluster $CLUSTER" 
else
   echo "4. The specified pod ($POD) contains $NODE_COUNT data nodes and the same number of members"
   echo "   have been added to the cluster. You can add or remove members by runnng 'add_member' or"
   echo "   'remove_member', respectively."
   echo "    IMPORTANT: The number of members cannot exceed the number of data nodes in the pod."
   echo "5. Once you have added or removed members, execute the following to configure and start the"
   echo "   Vagrant pod environment:"
   echo "      build_pod -pod $POD"
   echo "6. To get the pod details, run the following:"
   echo "      show_pod -pod $POD"
   echo "7. To use this pod, attach it to a cluster. For example,"
   echo "      create_cluster -cluster mycluster -pod $POD"
   echo "      start_cluster -cluster mycluster"
   echo "8. You can login to the primary node as follows."
   echo "      ssh vagrant@$NODE_NAME_PRIMARY"
fi
echo "----------------------------------------------------------------"
