

# ========================================================================
# Copyright (c) 2020-2023 Netcrest Technologies, LLC. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========================================================================

SCRIPT_DIR="$(cd -P -- "$(dirname -- "$0")" && pwd -P)"
. $SCRIPT_DIR/.addonenv.sh

# Set PRODUCT here. This is necessary due to the default product read from
# the RWE setenv.sh by the above .addonenv.sh
PRODUCT="hazelcast"

EXECUTABLE="`basename $0`"

__options()
{
   echo "-product -cluster -pod -vm -mc -port -?"
}

if [ "$OPTIONS" == "true" ]; then
   __options
   exit
fi

if [[ "$PRODUCT_ARG" == "" || "$PRODUCT_ARG" == "$PRODUCT" ]] && [ "$HELP" == "true" ]; then
cat <<EOF

WORKSPACE
   $PADOGRID_WORKSPACE

NAME
   $EXECUTABLE - Create a new cluster in the current workspace

SYNOPSIS
   $EXECUTABLE [-product product_name]
                 [-cluster cluster_name]
                 [-pod pod_name]
                 [-vm [comma_separated_hosts]]
                 [-mc management_center_host]
                 [-port first_port_number] [-?]

DESCRIPTION
   Creates a new cluster in the current workspace. Once the cluster is created,
   you can change the settings in the following file:

   etc/cluster.properties

OPTIONS
   -product
             Cluster product name. If unspecified then the current cluster's product is assigned.
             This command will abort if the specified product is not installed for the current
             workspace. To add or update product installations, run 'update_products' or set the
             appropriate '*_HOME' environment varibles in the RWE or workspace 'setenv.sh' file.
             Note that workspace 'setenv.sh' inherits RWE 'setenv.sh'.

   -cluster cluster_name
             Unique cluster name. The cluster name is prepended
             to all member names.

   -pod pod_name
             Pod name. The 'local' pod is the local machine. This option overrides the '-vm'
             option. 
             Default: local

   -vm [comma_separated_hosts]
             A list of VM hosts or addresses separated by comma. If the list
             contains spaces then enclosed it in quotes. If this option is not
             specified, then the host list defined by VM_HOSTS in the workspace
             'setenv.sh' file is applied.
             
             If the '-pod' option is specified then this option is suppressed.
             
   -mc management_center_host
             Management center host name. This option is meaningful only
             if the '-vm' option is specified. If this option is not
             specified but the '-vm' option is specified, then the first
             host from the '-vm' host list is selected for the management
             center.
             
   -port first_port_number
             First member's port number. Port number is incremented
             starting from this port number for the subsquent members.
             
             This option applies only for a cluster running locally. It is ignored
             for creating non-local pod and VM clusters.
             
             Default: 5701 for IMDG, 6701 for Jet
EOF
if [ "$MAN_SPECIFIED" == "false" ]; then
cat <<EOF
DEFAULT
   $EXECUTABLE -cluster $DEFAULT_CLUSTER -pod $POD -port $DEFAULT_MEMBER_START_PORT

FILES
   $PADOGRID_WORKSPACE/setenv.sh
             The current workspace configuration file.

EOF
fi
cat <<EOF
SEE ALSO
EOF
   printSeeAlsoList "*cluster*" $EXECUTABLE
   exit
fi

# Delegate the command to the specified product's create_cluster command.
COMMON_PRODUCT="$(getCommonProductName $PRODUCT_ARG)"
if [ "$COMMON_PRODUCT" != "$PRODUCT" ]; then
   COMMAND=$(getCreateClusterCommand $PRODUCT_ARG)
   if [ "$COMMAND" != "" ]; then
      $COMMAND "$@"
   fi
   exit
fi

# If -product is specified then make it default. This effectively
# provides a way to create any product clusters in the same workspace.
if [ "$PRODUCT_ARG" != "" ]; then
   PRODUCT=$PRODUCT_ARG
   if [ "$PRODUCT_ARG" == "jet" ]; then
      CLUSTER_TYPE="jet"
   fi
fi

# Set the default cluster name if not specified
if [ "$CLUSTER_ARG" == "" ]; then
   if [ "$PRODUCT" == "jet" ]; then
      CLUSTER=$DEFAULT_JET_CLUSTER
   else
      CLUSTER=$DEFAULT_CLUSTER
   fi
fi

if [ -z $CLUSTER ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: Cluster name is not specified. Command aborted." 
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

# Abort if the cluster exists
CLUSTER_DIR=$CLUSTERS_DIR/$CLUSTER
if [ -d $CLUSTER_DIR ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: Cluster already exists: [$CLUSTER]. Command aborted."
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

# Suppress pod if VM specified
#if [ "$VM_SPECIFIED" == "true" ]; then
#   POD="local"
#fi
if [ "$POD_SPECIFIED" == "false" ]; then
   POD="local"
fi
if [ "`isPodExist $POD`" == "false" ]; then
   echo >&2 "----------------------------------------------------------------"
   echo >&2 "WORKSPACE: $PADOGRID_WORKSPACE"
   echo >&2 "    ERROR: This pod does not exist [$POD]. Command aborted."
   echo >&2 "----------------------------------------------------------------"
   exit 1
fi

# Source in .padogrid/setenv.sh
SETENV_FILE="$HOME/.padogrid/setenv.sh"
if [ -f "$SETENV_FILE" ]; then
   . $SETENV_FILE
fi

#
# Check the license key
#
if [ "$IS_HAZELCAST_ENTERPRISE" == "true" ]; then
   if [ "$CLUSTER_TYPE" == "jet" ] && [ "$JET_LICENSE_KEY" == "" ] ||
      [ "$CLUSTER_TYPE" == "imdg" ] && [ "$IMDG_LICENSE_KEY" == "" ] ||
      [ "$CLUSTER_TYPE" == "hazelcast" ] && [ "$IMDG_LICENSE_KEY" == "" ]; then
      echo "WORKSPACE"
      echo "   $PADOGRID_WORKSPACE"
      echo ""
      echo "-----------------------------------------------------------------------------------"
      echo -e "${CBrownOrange}LICENSE KEY WARNING:${CNone}"
      echo "It seems you have not entered the enterprise license key. You can enter it in any"
      echo "of the following files. The higher in the list the wider it is applied. For example,"
      echo "setting it in '.hazecastenv.sh', applies to all workspaces in that directory."
      echo ""
      echo "   $PADOGRID_WORKSPACES_HOME/.hazelcastenv.sh"
      echo "   $PADOGRID_WORKSPACE/setenv.sh"
      echo "   $CLUSTER_DIR/bin_sh/setenv.sh"
      echo ""
      echo "The following is the recommended file to place your license key(s). This allows"
      echo "all workspaces to inherit the license key(s)."
      echo ""
      echo -e "${CLightRed}$PADOGRID_WORKSPACES_HOME/.hazelcastenv.sh${CNone}"
   fi
fi

function writeCluster() 
{
   if [ "$CLUSTER_TYPE" == "jet" ]; then
      # Update cluster env file with PRODUCT and CLUSTER_TYPE
      updateClusterEnvFile

      # Copy the template hazelcast.xml and hazelcast.yaml files
      cp $BASE_DIR/etc/template-jet-imdg-hazelcast-$HAZELCAST_MAJOR_VERSION_NUMBER.xml $ETC_DIR/hazelcast.xml
      #cp $BASE_DIR/etc/template-jet-imdg-hazelcast-$HAZELCAST_MAJOR_VERSION_NUMBER.yaml $ETC_DIR/hazelcast.yaml
      # Copy the template hazelcast-jet.xml file
      cp $BASE_DIR/etc/template-hazelcast-jet-$HAZELCAST_MAJOR_VERSION_NUMBER.xml $ETC_DIR/hazelcast-jet.xml
      # Copy the template Jet MC application.properties file
      cp $BASE_DIR/etc/template-jet-mc-application.properties $ETC_DIR/jet-mc-application.properties

      # Insert cluster member list in hazelcast-client.xml
      JET_CLUSTER_MEMBERS=""
      HOST_IP="localhost"
      HOST_SERVER_PORT=$NEW_MEMBER_START_PORT
      COUNT=2
      for i in $(seq 1 $COUNT); do   
         if [ $i -lt $COUNT ]; then
            JET_CLUSTER_MEMBERS="${JET_CLUSTER_MEMBERS} <address>${HOST_IP}:${HOST_SERVER_PORT}<\/address>"
         else
            JET_CLUSTER_MEMBERS="${JET_CLUSTER_MEMBERS} <address>${HOST_IP}:${HOST_SERVER_PORT}<\/address>"
         fi
         let HOST_SERVER_PORT=HOST_SERVER_PORT+1
      done
      sed -e "s/\${CLUSTER_MEMBERS}/$JET_CLUSTER_MEMBERS/g" $BASE_DIR/etc/template-hazelcast-jet-client-$HAZELCAST_MAJOR_VERSION_NUMBER.xml > $ETC_DIR/hazelcast-client.xml
   else
      if [ $HAZELCAST_MAJOR_VERSION_NUMBER -ge 5 ]; then
         # Change cluster type to hazelcast. "jet" and "imdg" are for keeping the cluster creation
         # steps consistent and for setting the initial config file only.
         CLUSTER_TYPE="hazelcast"
      fi

      # Update cluster env file with PRODUCT and CLUSTER_TYPE
      updateClusterEnvFile

      if [ $HAZELCAST_MAJOR_VERSION_NUMBER -eq 3 ] && [ $HAZELCAST_MINOR_VERSION_NUMBER -lt 12 ]; then
         cp  $BASE_DIR/etc/template-hazelcast-${HAZELCAST_MAJOR_VERSION_NUMBER}.x.xml $ETC_DIR/hazelcast.xml
         cp  $BASE_DIR/etc/template-hazelcast-${HAZELCAST_MAJOR_VERSION_NUMBER}.x.yaml $ETC_DIR/hazelcast.yaml
         cp  $BASE_DIR/etc/template-hazelcast-indexes-${HAZELCAST_MAJOR_VERSION_NUMBER}.x.xml $ETC_DIR/hazelcast-indexes.xml
         cp  $BASE_DIR/etc/template-hazelcast-indexes-${HAZELCAST_MAJOR_VERSION_NUMBER}.x.yaml $ETC_DIR/hazelcast-indexes.yaml
      else
         cp $BASE_DIR/etc/template-hazelcast-$HAZELCAST_MAJOR_VERSION_NUMBER.xml $ETC_DIR/hazelcast.xml
         cp $BASE_DIR/etc/template-hazelcast-$HAZELCAST_MAJOR_VERSION_NUMBER.yaml $ETC_DIR/hazelcast.yaml
         cp $BASE_DIR/etc/template-hazelcast-indexes-$HAZELCAST_MAJOR_VERSION_NUMBER.xml $ETC_DIR/hazelcast-indexes.xml
         cp $BASE_DIR/etc/template-hazelcast-indexes-$HAZELCAST_MAJOR_VERSION_NUMBER.yaml $ETC_DIR/hazelcast-indexes.yaml
       fi
   fi
   # Copy the template log4j2.properties file
   cp $BASE_DIR/etc/template-log4j2.properties $ETC_DIR/log4j2.properties
   # Copy the template prometheus config file
   cp $BASE_DIR/etc/template-prometheus.yml $ETC_DIR/prometheus.yml
   # Copy the template hibernate config file
   cp $BASE_DIR/etc/template-hibernate.cfg-mysql.xml $ETC_DIR/hibernate.cfg-mysql.xml
   cp $BASE_DIR/etc/template-hibernate.cfg-postgresql.xml $ETC_DIR/hibernate.cfg-postgresql.xml
   cp $BASE_DIR/etc/template-hibernate.cfg-derby.xml $ETC_DIR/hibernate.cfg-derby.xml

   # Create bin_sh/setenv.sh
   mkdir -p $CLUSTER_DIR/bin_sh
   echo "#" > $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Add cluster specific environment variables in this file." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Set Java options, i.e., -Dproperty=xyz" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# To use Hibernate backed MapStorePkDbImpl, set the following property and" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# configure MapStorePkDbImpl in the \$CLUSTER_DIR/etc/hazelcast.xml file." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# MySQL and PostgreSQL Hibernate configuration files are provided to get" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# you started. You should copy one of them and enter your DB information." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# You can include your JDBC driver in the ../pom.xml file and run ./build_app" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# which downloads and places it in the \$PADOGRID_WORKSPACE/lib" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# directory. CLASSPATH includes all the jar files in that directory for" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# the apps and clusters running in this workspace." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\"\$JAVA_OPTS -Dhazelcast-addon.hibernate.config=\$CLUSTER_DIR/etc/hibernate.cfg-mysql.xml\"" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\"\$JAVA_OPTS -Dhazelcast-addon.hibernate.config=\$CLUSTER_DIR/etc/hibernate.cfg-postgresql.xml\"" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#JAVA_OPTS=\"\$JAVA_OPTS -Dhazelcast-addon.hibernate.config=\$CLUSTER_DIR/etc/hibernate.cfg-derby.xml\"" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Set Management Center Java options, i.e., -Dhazelcast.mc.forceLogoutOnMultipleLogin=true" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#MC_JAVA_OPTS=" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo ""  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Set RUN_SCRIPT. Absolute path required." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# If set, the 'start_member' command will run this script instead of com.hazelcast.core.server.StartServer." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Your run script will inherit the following:" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    JAVA      - Java executable."  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    JAVA_OPTS - Java options set by padogrid." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    CLASSPATH - Class path set by padogrid. You can include additional libary paths." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#                You should, however, place your library files in the plugins directories" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#                if possible." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    CONFIG_FILE - Config file path. You can override the config file path." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Run Script Example:" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#    \"\$JAVA\" \$JAVA_OPTS com.newco.MyMember &" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#" >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "# Although it is not required, your script should be placed in the bin_sh directory." >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#"  >> $CLUSTER_DIR/bin_sh/setenv.sh
   echo "#RUN_SCRIPT=\$CLUSTER_DIR/bin_sh/your-script" >> $CLUSTER_DIR/bin_sh/setenv.sh
   
   # Create $ETC_DIR/cluster.properties
   echo "# padogrid cluster properties." > $ETC_DIR/cluster.properties
   echo "# These properties pertain to all of the members belonging to the named cluster below." >> $ETC_DIR/cluster.properties
   echo "# Cluster Creation Date: `date`" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Cluster
   echo "# Cluster name" >> $ETC_DIR/cluster.properties
   echo "cluster.name=$CLUSTER" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Pod
   echo "# Pod properties" >> $ETC_DIR/cluster.properties
   echo "pod.name=$POD" >> $ETC_DIR/cluster.properties
   echo "pod.type=$POD_TYPE" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties

   # Bind Address
   echo "# Bind to a specific IP address of a multi-home machine. By default, PadoGrid binds" >> $ETC_DIR/cluster.properties
   echo "# to `hostname` if this property is not specified." >> $ETC_DIR/cluster.properties
   echo "#bind.address=" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # TCP port
   echo "# Member TCP start port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "tcp.startPort=$NEW_MEMBER_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Debug
   echo "# Enable/disable debugging" >> $ETC_DIR/cluster.properties
   echo "debug.enabled=$DEFAULT_DEBUG_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's debug port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "debug.startPort=$NEW_DEBUG_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # JMX
   echo "# Enable/disable jmx" >> $ETC_DIR/cluster.properties
   echo "jmx.enabled=$DEFAULT_JMX_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's JMX port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "jmx.startPort=$NEW_JMX_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Prometheus
   echo "# Enable/disable Prometheus" >> $ETC_DIR/cluster.properties
   echo "prometheus.enabled=$DEFAULT_PROMETHEUS_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# The first member's PROMETHEUS port number. The port number is incremented starting from this number." >> $ETC_DIR/cluster.properties
   echo "prometheus.startPort=$NEW_PROMETHEUS_START_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Heap
   echo "# Heap min and max values" >> $ETC_DIR/cluster.properties
   echo "heap.min=$DEFAULT_MIN_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "heap.max=$DEFAULT_MAX_HEAP_SIZE" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # MC
   echo "# Management Center host and port numbers" >> $ETC_DIR/cluster.properties
   echo "mc.host=$MC_HOST" >> $ETC_DIR/cluster.properties
   echo "mc.http.port=$NEW_MC_HTTP_PORT" >> $ETC_DIR/cluster.properties
   echo "mc.https.port=$NEW_MC_HTTPS_PORT" >> $ETC_DIR/cluster.properties
   echo "mc.https.enabled=false" >> $ETC_DIR/cluster.properties
   echo "mc.jmx.port=$NEW_MC_JMX_PORT" >> $ETC_DIR/cluster.properties
   echo "mc.jmx.rmi.port=$NEW_MC_JMX_RMI_PORT" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Health Monitor logging
   echo "# Health Monitor logging flag. If true then Health Monitor information is logged." >> $ETC_DIR/cluster.properties
   echo "health.log.enabled=$DEFAULT_HEALTH_MONITOR_ENABLED" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # Diagnositics logging
   echo "# Diagnotics  logging flag. If true then Diagnostics information is logged." >> $ETC_DIR/cluster.properties
   echo "diagnostics.log.enabled=$DEFAULT_DIAGNOSTICS_ENABLED" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # GC logging
   echo "# GC logging flag. If true then GC information is logged." >> $ETC_DIR/cluster.properties
   echo "gc.log.enabled=$DEFAULT_GC_LOG_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# Default GC log file flag. If true then GC information is logged in a separate log file." >> $ETC_DIR/cluster.properties
   echo "gc.log.file.enabled=$DEFAULT_GC_LOG_FILE_ENABLED" >> $ETC_DIR/cluster.properties
   echo "" >> $ETC_DIR/cluster.properties
   
   # VM
   echo "# Enable/disable VM cluster" >> $ETC_DIR/cluster.properties
   echo "vm.enabled=$VM_ENABLED" >> $ETC_DIR/cluster.properties
   echo "# A comma separated list of host names or addresses. IMPORTANT: No spaces allowed." >> $ETC_DIR/cluster.properties
   VM_HOSTS=$(echo "$VM_HOSTS" | sed "s/ //g")
   echo "vm.hosts=$VM_HOSTS" >> $ETC_DIR/cluster.properties
   echo "# SSH user name. If not specified then defaults to the shell login session user name." >> $ETC_DIR/cluster.properties
   echo "vm.user=$VM_USER" >> $ETC_DIR/cluster.properties
   echo "# Optional private key file path. e.g., a private '.pem' key file. If not specified" >> $ETC_DIR/cluster.properties
   echo "# it defaults to VM_PRIVATE_KEY_FILE defined in the workspace 'vmenv.sh' file." >> $ETC_DIR/cluster.properties
   echo "#vm.privateKeyFile=$VM_KEY_FILE" >> $ETC_DIR/cluster.properties

   # Set group permissions for workspace owner
   if [ "$GROUP_PERMISSIONS_ENABLED" == "true" ]; then
      chmod -R g+rw,o-rwx $CLUSTER_DIR
   fi
}

# 
# VM cluster
#
if [ "$VM_SPECIFIED" == "true" ] && [ "$POD" == "local" ]; then
   VM_ENABLED="true"
fi
if [ "$VM_USER" == "" ]; then
   VM_USER=$(whoami)
fi
VM_KEY_FILE="/your/private/keyfile.pem"
if [ "$VM_ENABLED" == "true" ]; then
   if [ "$VM_HOSTS_ARG" != "" ]; then
      VM_HOSTS="$VM_HOSTS_ARG"
   fi
   if [ "$MC" == "" ]; then
      # Extract the first host from the VM host list
      VM_MC_HOSTS=${VM_HOSTS%%,*}
   else
      VM_MC_HOSTS=$MC
   fi
   if [ "$VM_MC_HOSTS" == "" ]; then
      echo "WARNING: VM management center (mc) host\(s\) undefined. Configuring with this host."
      VM_MC_HOSTS=`hostname`
   fi
   if [ "$VM_HOSTS" == "" ]; then
      echo "WARNING: VM hosts undefined. Configuring with this host."
      VM_HOSTS=`hostname`
   fi
else
   VM_MC_HOSTS=`hostname`
   VM_HOSTS=`hostname`
fi

# Collect all clusters
pushd $CLUSTERS_DIR > /dev/null 2>&1
CLUSTERS=""
COUNT=0
for i in *; do
   if [ -d "$i" ]; then
      let COUNT=COUNT+1
      if [ $COUNT -eq 1 ]; then
         CLUSTERS="$i"
      else
         CLUSTERS="$CLUSTERS $i"
      fi
   fi
done
popd > /dev/null 2>&1

# If the port number is not specified, or if there aren't any existing clusters
# or the pod name is not "local" then use the default ports. For non-local pods,
# we can ignore the port number conflicts.
if [ "$PORT_ARG" == "" ] || [ "$CLUSTER" == "" ] || [ "$POD" != "local" ] || [ "$VM_ENABLED" == "true" ]; then
   NEW_MEMBER_START_PORT=$DEFAULT_MEMBER_START_PORT
   NEW_DEBUG_START_PORT=$DEFAULT_DEBUG_START_PORT
   NEW_JMX_START_PORT=$DEFAULT_JMX_START_PORT
   NEW_PROMETHEUS_START_PORT=$DEFAULT_PROMETHEUS_START_PORT
   NEW_MC_HTTP_PORT=$DEFAULT_MC_HTTP_PORT
   NEW_MC_HTTPS_PORT=$DEFAULT_MC_HTTPS_PORT
   NEW_MC_JMX_PORT=$DEFAULT_MC_JMX_PORT
   NEW_MC_JMX_RMI_PORT=$DEFAULT_MC_JMX_RMI_PORT
else
   NEW_MEMBER_START_PORT=$PORT_ARG
   let PORT_DIFF=NEW_MEMBER_START_PORT-DEFAULT_MEMBER_START_PORT
   let NEW_DEBUG_START_PORT=DEFAULT_DEBUG_START_PORT+PORT_DIFF
   let NEW_JMX_START_PORT=DEFAULT_JMX_START_PORT+PORT_DIFF
   let NEW_PROMETHEUS_START_PORT=DEFAULT_PROMETHEUS_START_PORT+PORT_DIFF

   MEMBER_START_PORTS=""
   MC_HTTP_PORTS=""
   MC_HTTPS_PORTS=""
   MC_JMX_PORTS=""
   MC_JMX_RMI_PORTS=""
   PORTS_FOUND="false"
   for __CLUSTER in ${CLUSTERS}; do
      __CLUSTER_DIR=$CLUSTERS_DIR/$__CLUSTER
      __ETC_DIR=$__CLUSTER_DIR/etc
      CLUSTER_PROPERTIES_FILE=$__ETC_DIR/cluster.properties
      while IFS= read -r line; do
         line=`trimString $line`
         if [[ $line == tcp.startPort=* ]]; then
            MEMBER_START_PORT=${line##tcp.startPort=}
            MEMBER_START_PORTS="$MEMBER_START_PORTS $MEMBER_START_PORT"
         elif [[ $line == mc.http.port=* ]]; then
            MC_HTTP_PORT=${line##mc.http.port=}
            MC_HTTP_PORTS="$MC_HTTP_PORTS $MC_HTTP_PORT"
         elif [[ $line == mc.https.port=* ]]; then
            MC_HTTPS_PORT=${line##mc.https.port=}
            MC_HTTPS_PORTS="$MC_HTTPS_PORTS $MC_HTTPS_PORT"
         elif [[ $line == mc.jmx.port=* ]]; then
            MC_JMX_PORT=${line##mc.jmx.port=}
            MC_JMX_PORTS="$MC_JMX_PORTS $MC_JMX_PORT"
         elif [[ $line == mc.jmx.rmi.port=* ]]; then
            MC_JMX_RMI_PORT=${line##mc.jmx.rmi.port=}
            MC_JMX_RMI_PORTS="$MC_JMX_RMI_PORTS $MC_JMX_RMI_PORT"
         fi
      done < "$CLUSTER_PROPERTIES_FILE"
      if [ $MEMBER_START_PORT -eq $NEW_MEMBER_START_PORT ]; then
         NEW_MC_HTTP_PORT=$MC_HTTP_PORT
         NEW_MC_HTTPS_PORT=$MC_HTTPS_PORT
         NEW_MC_JMX_PORT=$MC_JMX_PORT
         NEW_MC_JMX_RMI_PORT=$MC_JMX_RMI_PORT
         PORTS_FOUND="true"
         break
      fi 
   done

   # Determine the member DEBUG start port number
   let MAX_PORT_INTERVAL=MAX_MEMBER_COUNT*2
   START_PORTS=`echo "$START_PORTS" | xargs -n1 | sort -u | xargs`
   let PREV_START_PORT=DEFAULT_DEBUG_START_PORT-MAX_MEMBER_COUNT
   if [ "$START_PORTS" == "" ]; then
      	START_PORTS=$PREV_START_PORT
   fi
   for START_PORT in ${START_PORTS}; do
      let interval=START_PORT-PREV_START_PORT
      if [ $interval -ge $MAX_PORT_INTERVAL ]; then
         let NEW_DEBUG_START_PORT=PREV_START_PORT+MAX_MEMBER_COUNT
         break;
      fi
      PREV_START_PORT=$START_PORT
   done
   if [ -z $NEW_DEBUG_START_PORT ]; then
      let NEW_DEBUG_START_PORT=PREV_START_PORT+MAX_MEMBER_COUNT
   fi

   # Set the port number to the next largest or smallest number
   if [ "$PORTS_FOUND" == "false" ]; then

      if [ $PORT_DIFF -gt 0 ]; then
         let INCREMENT=1 
      elif [ $PORT_DIFF -lt 0 ]; then
         let INCREMENT=-1
      else
         let INCREMENT=0
      fi
      
      # Determine the MC HTTP port number
      MC_HTTP_PORTS=`echo "$MC_HTTP_PORTS" | xargs -n1 | sort -u | xargs`
      NEW_MC_HTTP_PORT=""
      PREV_PORT=$DEFAULT_MC_HTTP_PORT
      for MC_HTTP_PORT in ${MC_HTTP_PORTS}; do
         let next_port=PREV_PORT+INCREMENT
         if [ $next_port -lt $MC_HTTP_PORT ]; then
            NEW_MC_HTTP_PORT=$next_port
            break;
         fi
         PREV_PORT=$MC_HTTP_PORT
      done
      if [ -z $NEW_MC_HTTP_PORT ]; then
         if [ -z $MC_HTTP_PORT ]; then
            NEW_MC_HTTP_PORT=$DEFAULT_MC_HTTP_PORT
         else
            let NEW_MC_HTTP_PORT=MC_HTTP_PORT+INCREMENT
         fi
      fi

      # Determine the MC HTTPS port number
      MC_HTTPS_PORTS=`echo "$MC_HTTPS_PORTS" | xargs -n1 | sort -u | xargs`
      NEW_MC_HTTPS_PORT=""
      PREV_PORT=$DEFAULT_MC_HTTPS_PORT
      for MC_HTTPS_PORT in ${MC_HTTPS_PORTS}; do
         let next_port=PREV_PORT+INCREMENT
         if [ $next_port -lt $MC_HTTPS_PORT ]; then
            NEW_MC_HTTPS_PORT=$next_port
            break;
         fi
         PREV_PORT=$MC_HTTPS_PORT
      done
      if [ -z $NEW_MC_HTTPS_PORT ]; then
         if [ -z $MC_HTTPS_PORT ]; then
            NEW_MC_HTTPS_PORT=$DEFAULT_MC_HTTPS_PORT
         else
            let NEW_MC_HTTPS_PORT=MC_HTTPS_PORT+INCREMENT
         fi
      fi
      
      # Determine the MC JMX port number
      MC_JMX_PORTS=`echo "$MC_JMX_PORTS" | xargs -n1 | sort -u | xargs`
      NEW_MC_JMX_PORT=""
      PREV_PORT=$DEFAULT_MC_JMX_PORT
      for MC_JMX_PORT in ${MC_JMX_PORTS}; do
         let next_port=PREV_PORT+INCREMENT
         if [ $next_port -lt $MC_JMX_PORT ]; then
            NEW_MC_JMX_PORT=$next_port
            break;
         fi
         PREV_PORT=$MC_JMX_PORT
      done
      if [ -z $NEW_MC_JMX_PORT ]; then
         if [ -z $MC_JMX_PORT ]; then
            NEW_MC_JMX_PORT=$DEFAULT_MC_JMX_PORT
         else
            let NEW_MC_JMX_PORT=MC_JMX_PORT+INCREMENT
         fi
      fi
      
      # Determine the MC JMX RMI port number
      MC_JMX_RMI_PORTS=`echo "$MC_JMX_RMI_PORTS" | xargs -n1 | sort -u | xargs`
      NEW_MC_JMX_RMI_PORT=""
      PREV_PORT=$DEFAULT_MC_JMX_RMI_PORT
      for MC_JMX_RMI_PORT in ${MC_JMX_RMI_PORTS}; do
         let next_port=PREV_PORT+INCREMENT
         if [ $next_port -lt $MC_JMX_RMI_PORT ]; then
            NEW_MC_JMX_RMI_PORT=$next_port
            break;
         fi
         PREV_PORT=$MC_JMX_RMI_PORT
      done
      if [ -z $NEW_MC_JMX_RMI_PORT ]; then
         if [ -z $MC_JMX_RMI_PORT ]; then
            NEW_MC_JMX_RMI_PORT=$DEFAULT_MC_JMX_RMI_PORT
         else
            let NEW_MC_JMX_RMI_PORT=MC_JMX_RMI_PORT+INCREMENT
         fi
      fi
   fi
fi

# Create the cluster sub-directories and files.
LOG_DIR=$CLUSTERS_DIR/$CLUSTER/log
ETC_DIR=$CLUSTERS_DIR/$CLUSTER/etc
LIB_DIR=$CLUSTERS_DIR/$CLUSTER/lib
PLUGINS_DIR=$CLUSTERS_DIR/$CLUSTER/plugins
CONFIG_FILE=$CLUSTERS_DIR/$CLUSTER/etc/hazelcast.xml
mkdir -p $ETC_DIR
mkdir -p $LOG_DIR
mkdir -p $LIB_DIR
mkdir -p $PLUGINS_DIR

# Add one (1) member if VM is enabled.
# Add two (2) members if the pod name is "local".
# For non-local clusters with remote members, we start with an empty cluster and
# allow the user to identify the VMs to run the members.
if [ "$VM_ENABLED" == "true" ]; then
   MEMBER_1=`getMemberName 1`
   MEMBER_1_DIR=$CLUSTER_DIR/run/$MEMBER_1
   mkdir -p $MEMBER_1_DIR
   if [ "POD" == "local" ]; then
      POD_TYPE="local"
   fi
   MC_HOST=$VM_MC_HOSTS
elif [ "$POD" == "local" ]; then
   MEMBER_1=`getMemberName 1`
   MEMBER_2=`getMemberName 2`
   MEMBER_1_DIR=$CLUSTER_DIR/run/$MEMBER_1
   MEMBER_2_DIR=$CLUSTER_DIR/run/$MEMBER_2
   mkdir -p $MEMBER_1_DIR
   mkdir -p $MEMBER_2_DIR
   POD_TYPE="local"
   MC_HOST="localhost"
fi

if [ "$POD" != "local" ]; then

   # TODO: Pod VM_ENABLED is currently for workspace-wide only. To support individual clusters,
   #       VM_* parameters must be set for each cluster. Significant overhauling may required.
   # VM_ENABLED="true"

   AVAHI_ENABLED=`getPodProperty "pod.avahi.enabled" false`
   NODE_COUNT=`getPodProperty "node.count" "0"`
   VM_USER="vagrant"
   POD_TYPE="vagrant"
   VM_HOSTS=""
   
   NODE_NAME_PREFIX=`getPodProperty "node.name.prefix" $NODE_NAME_PREFIX`
   for i in $(seq 1 $NODE_COUNT); do
       MEMBER=`getMemberName $i`
       MEMBER_DIR=$CLUSTER_DIR/run/$MEMBER
       mkdir -p $MEMBER_DIR
   done
   
   # Vagrant pod VMs
   if [ "$AVAHI_ENABLED" == "true" ]; then
      NODE_NAME_PRIMARY=`getPodProperty "node.name.primary"`
      if [ "$NODE_NAME_PRIMARY" != "" ]; then
         NODE_NAME_PRIMARY=${NODE_NAME_PRIMARY}.local
      else
         NODE_NAME_PRIMARY="localhost"
      fi
      MC_HOST=$NODE_NAME_PRIMARY

      for i in $(seq 1 $NODE_COUNT); do
        __MEMBER_NUM=`getMemberNumWithLeadingZero $i`
        NODE_LOCAL="${NODE_NAME_PREFIX}-${__MEMBER_NUM}.local"
        if [ $i -eq 1 ]; then
           VM_HOSTS="$NODE_LOCAL"
        else
           VM_HOSTS="$VM_HOSTS,$NODE_LOCAL"
        fi
      done
   else
      POD_DIR="$PADOGRID_WORKSPACE/pods/$POD"
      POD_SETENV="$POD_DIR/bin_sh/setenv.sh"
      HOST_PRIVATE_IP=""
      if [ -f "$POD_SETENV" ]; then
         . $POD_SETENV
      fi
      if [ "$HOST_PRIVATE_IP" == "" ]; then
         AVAILABLE_IP_ADDRESSES=`getPrivateNetworkAddresses`
         if [ "$AVAILABLE_IP_ADDRESSES" == "" ]; then
            echo >&2 "ERROR: Private IP address not specified and unable to determine. Please add"
            echo >&2 "       a host-only ethernet adapter to VirtualBox. Command aborted."
            exit 1
         fi
         # Pick the first one in the list
         for i in $AVAILABLE_IP_ADDRESSES; do
            HOST_PRIVATE_IP=$i
            break;
         done
      fi
      IP_LAST_OCTET=`getPodProperty "node.ip.lastOctet" $IP_LAST_OCTET`
      # Extract the first three octets from the primary IP.
      BASE_PRIVATE_IP_FIRST_THREE_OCTETS=${HOST_PRIVATE_IP%.*}
      BASE_PRIVATE_IP_LAST_OCTET=$IP_LAST_OCTET
      BASE_PRIVATE_IP=${BASE_PRIVATE_IP_FIRST_THREE_OCTETS}.${BASE_PRIVATE_IP_LAST_OCTET}
      NODE_NAME_PRIMARY=${BASE_PRIVATE_IP}
      MC_HOST=$NODE_NAME_PRIMARY
      for i in $(seq 1 $NODE_COUNT); do
         let NODE_OCTET=i+BASE_PRIVATE_IP_LAST_OCTET
         NODE_IP=${BASE_PRIVATE_IP_FIRST_THREE_OCTETS}.${NODE_OCTET}
         if [ $i -eq 1 ]; then
            VM_HOSTS="$NODE_IP"
         else
            VM_HOSTS="$VM_HOSTS,$NODE_IP"
         fi
      done
   fi

fi

# Write cluster files
writeCluster

# Create the cluster on all VMs by copying the cluster dir and adding members.
#if [ "$VM_ENABLED" == "true" ] && [ "$VM_HOSTS" != "" ]; then
#   # Copy the cluster dir to all other VMs
#   $BASE_DIR/bin_sh/vm_copy -vm $VM_HOSTS $CLUSTER_DIR 
#
#   # Add a member to all VMs
#   $BASE_DIR/bin_sh/add_member -cluster $CLUSTER -all
#fi

# Display cluster information
echo "-----------------------------------------------------------------------------------"
echo "            WORKSPACE: $PADOGRID_WORKSPACE"
echo "      Cluster Created: $CLUSTER"
echo "         CLUSTER_TYPE: $CLUSTER_TYPE"
if [ "$VM_ENABLED" == "true" ]; then
echo "           Deployment: VM"
else
echo "                  POD: $POD"
fi
echo "          CLUSTER_DIR: $CLUSTER_DIR"
echo "           CONFIG_DIR: $ETC_DIR"
echo "          CONFIG_FILE: $CONFIG_FILE"
echo "              LOG_DIR: $LOG_DIR"
echo "    MEMBER_START_PORT: $NEW_MEMBER_START_PORT"
echo "     DEBUG_START_PORT: $NEW_DEBUG_START_PORT"
echo "       JMX_START_PORT: $NEW_JMX_START_PORT"
echo "PROMETHEUS_START_PORT: $NEW_PROMETHEUS_START_PORT"
echo "         MC_HTTP_PORT: $NEW_MC_HTTP_PORT"
echo "        MC_HTTPS_PORT: $NEW_MC_HTTPS_PORT"
echo "          MC_JMX_PORT: $NEW_MC_JMX_PORT"
echo "      MC_JMX_RMI_PORT: $NEW_MC_JMX_RMI_PORT"
echo ""
echo "The default hazelcast.xml file has been placed in the CONFIG_DIR directory."
echo "Modify or replace it with the appropriate file."
echo ""
if [ "$VM_ENABLED" == "true" ] && [ "$POD" == "local" ]; then
   echo "One (1) member has been configured (added) to run in this cluster as follows:"
   echo ""
   echo "      Member: $MEMBER_1"
   echo " Working Dir: $MEMBER_1_DIR"
   echo ""
elif [ "$POD" == "local" ]; then
   echo "Two (2) members have been configured (added) to run in this cluster as follows:"
   echo ""
   echo "      Member: $MEMBER_1"
   echo " Working Dir: $MEMBER_1_DIR"
   echo "      Member: $MEMBER_2"
   echo " Working Dir: $MEMBER_2_DIR"
   echo ""
else
   echo "$NODE_COUNT members have been configured (added) to run in this cluster as follows:"
   echo ""
   for i in $(seq 1 $NODE_COUNT); do
       MEMBER=`getMemberName $i`
       MEMBER_DIR=$CLUSTER_DIR/run/$MEMBER
      echo "      Member: $MEMBER"
      echo " Working Dir: $MEMBER_DIR"
   done
   echo ""
fi
echo "1. Update Hazelcast configuration in hazelcast.xml as needed:"
echo "      ${ETC_DIR}/hazelcast.xml"
echo "2. Place your application jar files in one of the 'plugins' directories:"
echo "      $PADOGRID_WORKSPACE/plugins/"
echo "      $CLUSTER_DIR/plugins/"
echo "3. Configure cluster properties in:"
echo "      $ETC_DIR/cluster.properties"
if [ "$VM_ENABLED" == "true" ]; then
   echo "4. You must configure password-less ssh login to all VMs. On AWS, this"
   echo "   is already configured but you will need to enter the correct private key"
   echo "   file path in the cluster.properties file."
   echo "5. To change VM host names, list the VM host names in the cluster.properties file"
   echo "   for management center and members."
   echo "      mc.host=host1"
   echo "      vm.hosts=host1,host2,..."
   echo "6. Deploy the cluster configuration changes you made to all the VM hosts listed in the"
   echo "   previous step by running 'vm_copy' or 'vm_sync'."
   echo "      vm_copy $CLUSTER_DIR"
   echo "7. To start the cluster:"
   echo "      start_cluster -cluster $CLUSTER"
   echo "8. To verify the cluster state:"
   echo "      show_cluster -cluster $CLUSTER"
   echo "9. To view log file:"
   echo "      show_log -cluster $CLUSTER" 
elif [ "$POD" == "local" ]; then 
   echo "4. To configure additional members:"
   echo "      add_member -cluster $CLUSTER"
   echo "5. To start the cluster: "
   echo "      start_cluster -cluster $CLUSTER"
   echo "6. To verify the cluster state:"
   echo "      show_cluster -cluster $CLUSTER"
   echo "7. To view log file:"
   echo "      show_log -cluster $CLUSTER" 
else
   echo "4. The specified pod ($POD) contains $NODE_COUNT data nodes and the same number of members"
   echo "   have been added to the cluster. You can add or remove members by runnng 'add_member' or"
   echo "   'remove_member', respectively."
   echo "    IMPORTANT: The number of members cannot exceed the number of data nodes in the pod."
   echo "5. Once you have added or removed members, execute the following to configure and start the"
   echo "   Vagrant pod environment:"
   echo "      build_pod -pod $POD"
   echo "6. To get the pod details, run the following:"
   echo "      show_pod -pod $POD"
   echo "7. To use this pod, attach it to a cluster. For example,"
   echo "      create_cluster -cluster mycluster -pod $POD"
   echo "      start_cluster -cluster mycluster"
   echo "8. You can login to the primary node as follows."
   echo "      ssh vagrant@$NODE_NAME_PRIMARY"
fi
echo ""
echo "To set this cluster as the current context, execute 'switch_cluster' as follows:"
echo -e "   ${CGo}switch_cluster $CLUSTER${CNone}"
echo "-----------------------------------------------------------------------------------"
